{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import networkx as nx\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import ndlib.models.epidemics as ep\n",
    "import ndlib.models.ModelConfig as mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some set of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_datasets = '../datasets/'\n",
    "path_to_uniform_data = '../data/'\n",
    "path_to_output = '../output/'\n",
    "path_to_samples = '../samples/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph_data(filename, sep=',', header=None, skiprows=0):\n",
    "    edgelist = pd.read_csv(os.path.join(path_to_datasets, filename), sep=sep, skiprows=skiprows, header=header, names=['source', 'target'])\n",
    "    edgelist.to_csv(os.path.join(path_to_uniform_data, filename), index=False, header=None)\n",
    "    return nx.from_pandas_edgelist(edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citeseer = read_graph_data('citeseer.cites', sep='\\t')\n",
    "deezer_europe = read_graph_data('deezer_europe_edges.csv', header=0)\n",
    "lastfm_asia = read_graph_data('lastfm_asia_edges.csv', header=0)\n",
    "cora = read_graph_data('cora.cites', sep='\\t')\n",
    "email_Eu_core = read_graph_data('email-Eu-core.txt', sep=' ')\n",
    "fb_0 = read_graph_data('0.edges', sep=' ')\n",
    "fb_1 = read_graph_data('107.edges', sep=' ')\n",
    "fb_2 = read_graph_data('348.edges', sep=' ')\n",
    "fb_3 = read_graph_data('414.edges', sep=' ')\n",
    "fb_4 = read_graph_data('686.edges', sep=' ')\n",
    "fb_5 = read_graph_data('698.edges', sep=' ')\n",
    "fb_6 = read_graph_data('1684.edges', sep=' ')\n",
    "fb_7 = read_graph_data('1912.edges', sep=' ')\n",
    "fb_8 = read_graph_data('3437.edges', sep=' ')\n",
    "fb_9 = read_graph_data('3980.edges', sep=' ')\n",
    "email_univ = read_graph_data('email-univ.edges', sep=' ')\n",
    "fb_company = read_graph_data('fb-pages-company.edges', header=0)\n",
    "fb_food = read_graph_data('fb-pages-food.edges')\n",
    "fb_politician = read_graph_data('fb-pages-politician.edges')\n",
    "fb_public_figure = read_graph_data('fb-pages-politician.edges')\n",
    "fb_tvshow = read_graph_data('fb-pages-tvshow.edges')\n",
    "soc_anybeat = read_graph_data('soc-anybeat.edges', sep=' ')\n",
    "soc_hamsterster = read_graph_data('soc-hamsterster.edges', sep=' ', skiprows=2)\n",
    "soc_wiki_vote = read_graph_data('soc-wiki-Vote.mtx', sep=' ', skiprows=2)\n",
    "cit_DBLP = read_graph_data('cit-DBLP.edges', sep=' ', skiprows=2)\n",
    "\n",
    "# list_of_graphs = [citeseer, deezer_europe, lastfm_asia, cora, email_Eu_core,\n",
    "#                   fb_0, fb_1, fb_2, fb_3, fb_4, fb_5, fb_6, fb_7, fb_8, fb_9,\n",
    "#                   email_univ, fb_company, fb_food, fb_politician, fb_public_figure,\n",
    "#                   fb_tvshow, soc_anybeat, soc_hamsterster, soc_wiki_vote, cit_DBLP]\n",
    "# len(list_of_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_list = os.listdir(path_to_uniform_data) # перевели все в 1 формат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_list = [] # считаем для каждого из них ndlib количество итераций до заражения\n",
    "for elem in tqdm(graphs_list): # make it with multiprocessing\n",
    "    iters_list.append((elem, calc_iter(elem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in tqdm(graphs_list): # считаем для них распределение мотивов\n",
    "    gt.extract_motifs(elem, 4, path_to_graphs=path_to_uniform_data, path_to_output=path_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in tqdm(graphs_list): # сэмплируем из них 10 графов с примерно половиной количества нод\n",
    "    \n",
    "    G = nx.from_pandas_edgelist(\n",
    "        pd.read_csv(os.path.join(path_to_uniform_data, graph), names=['source', 'target']))\n",
    "    \n",
    "    for j in range(20):\n",
    "        extra_hop = set()\n",
    "        first_node = np.random.choice(G.nodes())\n",
    "        extra_hop = extra_hop.union(list(nx.neighbors(G, first_node)))\n",
    "        i=1\n",
    "\n",
    "        while (i<4) and (len(extra_hop)<G.number_of_nodes()):\n",
    "            i+=1\n",
    "            for node in extra_hop:\n",
    "                 extra_hop = extra_hop.union(nx.neighbors(G, node))\n",
    "\n",
    "            nx.to_pandas_edgelist(nx.subgraph(G, extra_hop)).to_csv(\n",
    "            os.path.join(path_to_samples, graph+'_h{}_s{}.csv'.format(i,j+1)),\n",
    "            header=None,\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для каждого из них считаем распределения и усредняем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем датасет и делаем кроссвалидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iter(elem):    \n",
    "    g = nx.from_pandas_edgelist(\n",
    "    pd.read_csv(os.path.join(path_to_uniform_data, elem),\n",
    "                names=['source', 'target']))\n",
    "    \n",
    "    len_nodes = len(g.nodes())\n",
    "    \n",
    "    list_of_iter = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        model = ep.SIModel(g)\n",
    "        cfg = mc.Configuration()\n",
    "        cfg.add_model_parameter('beta', 0.1)\n",
    "        cfg.add_model_parameter(\"percentage_infected\", 0.01)\n",
    "        model.set_initial_status(cfg)\n",
    "    \n",
    "        iteration = model.iteration()\n",
    "    \n",
    "        while (iteration['node_count'][1]<len_nodes):\n",
    "            iteration = model.iteration()\n",
    "        \n",
    "        list_of_iter.append(iteration['iteration'])\n",
    "        \n",
    "    return elem, np.mean(list_of_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_size_n_dens(mdf):\n",
    "    path_to_gph = '/Users/zaikoval/Downloads/graphs_5types/graphs'\n",
    "    for idx in mdf.index:\n",
    "        graph = nx.from_pandas_edgelist(pd.read_csv(os.path.join(path_to_gph, '-'+str(idx)+'.csv'), names=['source', 'target']))\n",
    "        n = graph.number_of_nodes()\n",
    "        e = graph.number_of_edges()\n",
    "        mdf.loc[idx, 'nodes'] = n\n",
    "        mdf.loc[idx, 'edges'] = e\n",
    "        mdf.loc[idx, 'density'] = 2*e/(n*(n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTscanner:\n",
    "    \"\"\"\n",
    "    Python wrapper for GTscanner algorithm\n",
    "    \"\"\" \n",
    "    \n",
    "    def extract_motifs(self, filename, size, random=10, algo='fase', threads=3, path_to_graphs='', path_to_output='/Users/zaikoval/Documents/GitHub/network-motif-analysis/results/'):\n",
    "        \"\"\"\n",
    "        Calls execution of GTscanner algorithm with parameters:\n",
    "        \n",
    "        graph - path to the txt file of graph\n",
    "        \n",
    "        size - size of motif to extract \n",
    "        \n",
    "        random - number of random graph to generate (better 100+)\n",
    "        \"\"\"\n",
    "        import os\n",
    "        \n",
    "        path_to_examples = '/Users/zaikoval/Downloads/gtscanner/examples/sampling/'\n",
    "        graph_data = pd.read_csv(path_to_graphs+filename+'.csv', header=None)\n",
    "        unq_elem = np.unique(graph_data)\n",
    "        graph_data = graph_data.applymap(lambda x: np.where(x == unq_elem)[0][0]+1)\n",
    "        graph_data.to_csv(path_to_examples+filename+'.txt', sep=' ', header=None, index=False)\n",
    "        ы\n",
    "        cmd = '/home/zaikoval/Documents/Work/gtscanner/./GTScanner -s ' \\\n",
    "        + str(size) \\\n",
    "        + ' -m ' + algo \\\n",
    "        + ' -g ' + path_to_examples+filename+'.txt' \\\n",
    "        + ' -f simple' \\\n",
    "        + ' -t html' \\\n",
    "        + ' -o ' + output + filename + '_' + str(size) +'.html'\\\n",
    "        + ' -r ' + str(random) \\\n",
    "        + ' -th ' + str(threads)\n",
    "        #answer = os.popen(cmd).read()\n",
    "        print(cmd)\n",
    "    \n",
    "    def extract_result(self, file='/home/zaikoval/Documents/Work/gtscanner/results/result.html'):\n",
    "        from bs4 import BeautifulSoup\n",
    "        \n",
    "        adjs = []\n",
    "        freqs = []\n",
    "        zs = []\n",
    "        \n",
    "        soup = BeautifulSoup(open(file).read())\n",
    "        content = soup.find_all('tr')[1:]\n",
    "        \n",
    "        for motif in content:\n",
    "            adjs.append(np.matrix([list(x) for x in motif.find('td', attrs={'class':'pre'}).text.split('\\n')], dtype=int))\n",
    "            stats = motif.find_all('td')[2:4]\n",
    "            freqs.append(float(stats[0].text))\n",
    "            zs.append(float(stats[1].text))\n",
    "        \n",
    "        ans = list(zip(adjs, freqs, zs))\n",
    "        return ans\n",
    "    def data_4(self, files=[]):\n",
    "       \n",
    "        stats = []\n",
    "#         path_to_results_4 = '/Users/zaikoval/Documents/GitHub/network-motif-analysis/sampling/hop_result_4/'\n",
    "        \n",
    "        for item in files:\n",
    "            stats.append((item.split('/')[-1][:-7],\n",
    "                          self.extract_result(item)))\n",
    "            \n",
    "#         for item in stats:\n",
    "#             item.sort(key=lambda x: int(''.join(list(np.array(x[0]).flatten().astype(str))), base=10))\n",
    "            \n",
    "        pickleFile = open(\"/Users/zaikoval/Documents/GitHub/network-motif-analysis/dict_4.pkl\", 'rb')\n",
    "        dict_4 = pickle.load(pickleFile)\n",
    "        pickleFile.close()\n",
    "        \n",
    "        \n",
    "        i=0\n",
    "        \n",
    "        box_list = []\n",
    "        \n",
    "        for graph in stats:\n",
    "            \n",
    "            dict_4_z = dict(dict_4) \n",
    "            dict_4_f = dict(dict_4)\n",
    "            \n",
    "            \n",
    "            # for 4-motif\n",
    "            for elem in graph[1]:\n",
    "                dict_4_z[str(elem[0])] = 0\n",
    "                dict_4_f[str(elem[0])] = 0\n",
    "                \n",
    "                if (elem[2] not in [float('inf'), float('-inf')]) and (not np.isnan(elem[2])):\n",
    "                    dict_4_z[str(elem[0])] = elem[2]\n",
    "                else: \n",
    "                    dict_4_z[str(elem[0])] = 0\n",
    "                    \n",
    "                if (elem[1] not in [float('inf'), float('-inf')]) and (not np.isnan(elem[2])):\n",
    "                    dict_4_f[str(elem[0])] = elem[1]\n",
    "                else: \n",
    "                    dict_4_f[str(elem[0])] = 0\n",
    "            \n",
    "            z_scores_4 = list(dict_4_z.values())\n",
    "            freqs_4 = list(dict_4_f.values())\n",
    "            \n",
    "            sum_of_freqs_4 = np.sum(freqs_4)\n",
    "            normed_freqs_4 = freqs_4 / sum_of_freqs_4\n",
    "            \n",
    "            normed_z_scores_4 = z_scores_4 / np.sqrt(np.sum([x**2 for x in z_scores_4])) # ыыыыыыыыы\n",
    "         \n",
    "            \n",
    "            box_list.append([graph[0]] + \\\n",
    "                            #list(normed_z_scores_4) + \\\n",
    "                            list(normed_freqs_4))\n",
    "            \n",
    "        return box_list\n",
    "    \n",
    "gt = GTscanner()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
